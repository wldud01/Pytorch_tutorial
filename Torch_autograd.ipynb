{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2QzD4VRdeeeHWsVZk8syO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wldud01/Pytorch_tutorial/blob/main/Torch_autograd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### A Gentle Introduction to torch.autograd\n",
        "torch.autograd is pytorch automatic differenctiation engine that powers neural network training.\n",
        "\n",
        "- Background\n",
        "NNs fare collection of nested functions that aree executed on some input data. These functions are defined by parameters, which in Pytorch are stored in tensor.\n",
        "  - Trainig a NN happens in two steps\n",
        "\n",
        "  Forward Propagation: make its best guess about the correct output. It runs the input data through each of its functions to make this guess.\n",
        "\n",
        "  Backward Propagation: NNs adjusts its parameters proportionate to the error in its guess. It des this by traversing backwards of the functions(gradients), and optimizing the parameters using gradient descent."
      ],
      "metadata": {
        "id": "KBF5V8DdT28Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QcJWg7ZZTzvl"
      },
      "outputs": [],
      "source": [
        "#@title Usage in pytorch\n",
        "# load a pretrained model resnet 18 model from torchvision\n",
        "# create a random data tensor to represent a single image with 3 channels\n",
        "# Label in pretrained models has shape(1,1000)\n",
        "import torch\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
        "# 1 image 3 chanel size 64\n",
        "data = torch.rand(1,3,64,64)\n",
        "labels = torch.rand(1,1000)\n",
        "# RAM 1.4GB"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eSC6tnNWalG",
        "outputId": "290dd261-705c-40b1-db2e-c9fbed965ca6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title prediction\n",
        "# input data through the model - forward pass\n",
        "prediction =  model(data)"
      ],
      "metadata": {
        "id": "tZHOomM0Wpzw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title loss\n",
        "# caculate the error\n",
        "# to backpropagate this error through the network\n",
        "# kicked off when we call .backwards() on the error tensor\n",
        "# Autograd calculates and stores the gradients for each model parameter in the parameters .grad attribute\n",
        "loss = (prediction - labels).sum()\n",
        "loss.backward() # backward pass"
      ],
      "metadata": {
        "id": "gNE7r-QkW7hS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# each model paramter in the parameter's .grad attribute\n",
        "loss.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVthlonrYOMs",
        "outputId": "a4938fe3-4097-40dc-eaef-880abdde0e81"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-09a6243c8d35>:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
            "  loss.grad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title optimization\n",
        "# we register all the parameters of the model in the optimizer\n",
        "optim = torch.optim.SGD(model.parameters(),lr=1e-2, momentum =0.9)"
      ],
      "metadata": {
        "id": "5lnHs312XJMQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optim.step() # gradient descent\n",
        "# optimizer adjusts each parameters by its gradient stored in . grad"
      ],
      "metadata": {
        "id": "ohooy1bWYren"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Differentiation in Autograd\n",
        "# take a look at how autograd collects gradients\n",
        "# tensor a, b  required grad = True ,\n",
        "# this signal to autograd that every operation on them should be tracked\n",
        "\n",
        "import torch\n",
        "# Input vector\n",
        "a = torch.tensor([2.,3.],requires_grad = True)\n",
        "b = torch.tensor([6.,4.], requires_grad=True)"
      ],
      "metadata": {
        "id": "VfZe6vyVZD7-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create another tensor Q\n",
        "# like loss function\n",
        "Q = 3*a**3-b**2"
      ],
      "metadata": {
        "id": "VlerpquQZuIF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a,b is parameters and Q is to be the error\n",
        "# when we call .backward() on Q, autograd calculates these gradients and stores the,\n",
        "# gradient is a tensor of the same shape as Q, and it represents the gradient of Q itself\n",
        "external_grad = torch.tensor([1.,1.])\n",
        "Q.backward(gradient=external_grad)"
      ],
      "metadata": {
        "id": "VRWYQWoLZ_Dt"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(9*a**2 == a.grad)\n",
        "print(-2*b == b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSWPXgk2bVGk",
        "outputId": "30f295da-5157-4c4f-8cb4-166ce4169ae1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([True, True])\n",
            "tensor([True, True])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "@title vector caculus using autograd\n",
        "jacobian product가 each parameters differenciate\n",
        "Generally speaking torch.autograd isf an engine for computing vector-Jacobian product\n",
        "\n",
        "The backward pass kicks off when .backward() is called on the DAG root.\n",
        "- Compute the gradients from each .grad_fn\n",
        "- accumulates them in the respective tensor's .grad attribute and\n",
        "- using the chain rule, propagatess all the way to the leaf tensors\n",
        "\n"
      ],
      "metadata": {
        "id": "Pw9ilV2sOD-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Exclusion from the DAG\n",
        "import torch\n",
        "\n",
        "x = torch.rand(5,5)\n",
        "y = torch.rand(5,5)\n",
        "z = torch.rand((5,5),requires_grad=True)\n",
        "\n",
        "a = x+y\n",
        "# frozen parameters\n",
        "print(a.requires_grad)\n",
        "b = x +z\n",
        "print(b.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjNaZ-_ObdTC",
        "outputId": "97294624-1991-4d2e-834b-e0a373070bd1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In finetuning , we greeze most of the model\n",
        "# and only modify the classifier layers to make predictions\n",
        "\n",
        "from torch import nn, optim\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "\n",
        "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
        "\n",
        "# freeze all parameters\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sf5dKD8jPYFL",
        "outputId": "19fa40b2-3152-4299-ff2f-7b492bb135b7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 121MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fine tuning the model on a anew dataset with 10 labels\n",
        "# model.fc is the las linear layer\n",
        "# this code can simply replace it with a new linear layer\n",
        "model.fc = nn.Linear(512,10)"
      ],
      "metadata": {
        "id": "pEVxVOPuQKhS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-2, momentum = 0.9)"
      ],
      "metadata": {
        "id": "8EX3pc-OQufy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OMWE4diuRCRp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}